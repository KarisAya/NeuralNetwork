{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da6532a4-6df9-4a81-abd9-19e72a13f65f",
   "metadata": {},
   "source": [
    "# 编解码器模型 Encoder-Decoder Model\n",
    "\n",
    "随着RNN/LSTM在序列建模中显示出的潜力，研究者开始探索如何将这种能力应用于输入输出都是序列的任务。\n",
    "\n",
    "2014年，Cho等人提出了基于RNN的编码器-解码器框架用于机器翻译，同年Sutskever等人改进了这一架构并应用于神经机器翻译(NMT)\n",
    "\n",
    "标志着编解码器模型正式成为序列到序列(Seq2Seq)学习的主流框架。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6e1d4b-a5b1-4c34-941e-9cedfe508467",
   "metadata": {},
   "source": [
    "- 编解码器模型的基本原理\n",
    "\n",
    "    编解码器模型由两个主要组件构成：\n",
    "    \n",
    "    - 编码器(Encoder)：将输入序列（如源语言句子）编码为一个固定维度的上下文向量(context vector)\n",
    "    \n",
    "    - 解码器(Decoder)：基于上下文向量生成输出序列（如目标语言句子）\n",
    "    \n",
    "    编码阶段：输入序列通过编码器（通常是RNN/LSTM/GRU）逐步处理，最终生成一个浓缩输入信息的上下文向量\n",
    "    \n",
    "    状态传递：将上下文向量传递给解码器作为初始状态\n",
    "    \n",
    "    解码阶段：解码器逐步生成输出序列，通常使用自回归方式（前一步输出作为当前步输入）\n",
    "\n",
    "\n",
    "- 编解码器模型解决的核心问题\n",
    "\n",
    "    变长序列处理：突破了传统神经网络固定输入输出维度的限制，能够处理任意长度的序列数据。\n",
    "    \n",
    "    端到端学习：在机器翻译等任务中，取代了复杂的多阶段流水线，实现了从源语言到目标语言的直接映射学习。\n",
    "    \n",
    "    语义表示学习：编码器能够将输入序列编码为稠密的分布式表示，捕捉深层语义特征。\n",
    "    \n",
    "    跨模态转换：不仅限于文本，还可应用于语音-文本、图像-文本等不同模态间的转换任务。\n",
    "\n",
    "下面会实现一个简单的编解码器模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d642047d-7b28-4394-a167-f6df442d3a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b501413-f1e3-4cb9-b615-d0e0d7ab225b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, input_vs: int, emb_size: int, hidden_size: int, num_layers: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.Embedding(input_vs, emb_size)\n",
    "        self.lstm = torch.nn.LSTM(emb_size, hidden_size, num_layers, dropout=dropout, batch_first=True)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        embed = self.embedding(x)\n",
    "        outputs, state = self.lstm(embed)\n",
    "        return outputs, state\n",
    "\n",
    "\n",
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self, output_vs: int, emb_size: int, hidden_size: int, num_layers: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.Embedding(output_vs, emb_size)\n",
    "        self.lstm = torch.nn.LSTM(emb_size, hidden_size, num_layers, dropout=dropout, batch_first=True)\n",
    "        self.fc = torch.nn.Linear(hidden_size, output_vs)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, state: tuple[torch.Tensor, torch.Tensor]):\n",
    "        embed = self.embedding(x)\n",
    "        output, state = self.lstm(embed, state)\n",
    "        decoded = self.fc(output)\n",
    "        return decoded, state\n",
    "\n",
    "\n",
    "class EDM(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_vs: int,\n",
    "        output_vs: int,\n",
    "        emb_size: int,\n",
    "        hidden_size: int,\n",
    "        num_layers: int,\n",
    "        dropout: float = 0,\n",
    "        teacher_forcing_ratio: float = 0.5,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(input_vs, emb_size, hidden_size, num_layers, dropout)\n",
    "        self.decoder = Decoder(output_vs, emb_size, hidden_size, num_layers, dropout)\n",
    "        self.teacher_forcing_ratio = torch.tensor(teacher_forcing_ratio)\n",
    "\n",
    "    def forward(self, source: torch.Tensor, target: torch.Tensor):\n",
    "        _, state = self.encoder(source)\n",
    "        outputs = []\n",
    "        x = target[:, 0].unsqueeze(-1)\n",
    "        for target_t in target.unbind(1):\n",
    "            output, state = self.decoder(x, state)\n",
    "            outputs.append(output)\n",
    "            if torch.rand(1) < self.teacher_forcing_ratio:\n",
    "                x = target_t.unsqueeze(-1)\n",
    "            else:\n",
    "                x = output.argmax(-1)\n",
    "        return torch.cat(outputs, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c886b86-1ef0-4efe-a8b9-932a3886f1d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 15, 200])\n"
     ]
    }
   ],
   "source": [
    "model = EDM(100, 200, 50, 64, 2, 0.1)\n",
    "\n",
    "source = torch.randint(0, 100, (32, 10))\n",
    "target = torch.randint(0, 100, (32, 15))\n",
    "\n",
    "outputs = model(source, target)\n",
    "\n",
    "print(outputs.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9e22c9e8-4f1a-4d8e-b5cc-3f69ea30ce93",
   "metadata": {},
   "source": [
    "# 注意力机制\n",
    "\n",
    "注意力机制是基于循环神经网络的一种改进。在上面的例子中，我们可以看到编码器的输出完全没有使用。\n",
    "\n",
    "由于循环神经网络每一层的隐藏状态都对应了输入直到当前的语义，但在最后输出时却只用到了最后一个隐藏状态，这是一种浪费。\n",
    "\n",
    "而且，隐藏状态每一层都乘以当前权重，几层之后前文的影响会变得非常小。为了解决这个问题，人们首先提出了长短时记忆神经网络。\n",
    "\n",
    "人们很快意识到，词元的权重其实并不取决于它到当前词元的距离，所以在循环神经网络中，人们把每个词元的隐藏状态经过**加权平均**，形成一个统领全局的语义向量。\n",
    "\n",
    "这个全局向量就是注意力。\n",
    "\n",
    "# 注意力机制的实现\n",
    "\n",
    "输入数据经过编码器时把每一步的隐藏状态记录下来，我们会得到一个 T 个隐藏状态\n",
    "\n",
    "每个 token 都对应一个编码后的数据**输入隐藏状态**。最后一个隐藏状态是解码器的初始输入，这个是经典循环神经网络的实现机制\n",
    "\n",
    "经典 RNN 解码器的输入是从空隐藏状态开始逐个输入**输入隐藏状态**，然后输出的隐藏状态传给下个循环\n",
    "\n",
    "为了解决 RNN 固有的缺陷，人们在此基础上提出了注意力机制\n",
    "\n",
    "也就是把**全部输入隐藏状态**经过**加权**平均后的结果传进编码器的隐藏状态里，让这个神经网络着重注意更相关的词\n",
    "\n",
    "# 为什么计算加权和是通过张量矩阵乘法实现的？\n",
    "\n",
    "这个问题来自于经典RNN注意力机制中的编码器隐藏状态和和解码器隐藏状态进行内积计算对齐分数\n",
    "\n",
    "在上面的例子里只看编解码器隐藏状态 B 维度里的一个张量 编码器隐藏状态 enc_hids（大小为 [T,H]） 和 当前输入的解码器隐藏状态 dec_hid （大小为 [H]）\n",
    "\n",
    "*暂令 B=3，H=5*\n",
    "```\n",
    "enc_hids = [\n",
    "    [E11,E12,E13,E14,E15],\n",
    "    [E21,E22,E23,E24,E25],\n",
    "    [E31,E32,E33,E34,E35],\n",
    "]\n",
    "dec_hid = [D11,D12,D13,D14,D15]\n",
    "```\n",
    "enc_hids 的每一行都是编码器 token 的隐藏状态。\n",
    "\n",
    "设计思路是解码器的隐藏状态应该给和自己更相似的且更重要的 tokrn 更高权重。\n",
    "\n",
    "因此通常计算内积比较合适。\n",
    "\n",
    "把隐藏状态看作向量，内积比较大的情况满足两个向量方向相似，并且两个向量乘积本身较大 （类比 力*位移=做功 给做功大的位移的更大权重）\n",
    "\n",
    "则权重应该如此计算\n",
    "\n",
    "```\n",
    "S1 = E11*D11 + E12*D12 + ... + E1n*D1n\n",
    "S2 = E21*D11 + E22*D12 + ... + E2n*D1n\n",
    ".\n",
    ".\n",
    ".\n",
    "Sn = En1*D11 + En2*D12 + ... + Enn*D1n\n",
    "```\n",
    "dec_hid 和转置的 enc_hids 矩阵相乘刚好能得到权重矩阵\n",
    "```\n",
    "[D11,D12,D13,D14,D15] @ [\n",
    "    [E11,E21,E31],\n",
    "    [E12,E22,E32],\n",
    "    [E13,E23,E33],\n",
    "    [E14,E24,E34],\n",
    "    [E15,E25,E35],\n",
    "] = [S1,S2,S3]\n",
    "```\n",
    "权重矩阵和 enc_hids 矩阵相乘又刚好是加权结果\n",
    "```\n",
    "[S1,S2,S3] @ [\n",
    "    [E11,E12,E13,E14,E15],\n",
    "    [E21,E22,E23,E24,E25],\n",
    "    [E31,E32,E33,E34,E35],\n",
    "] = [\n",
    "    S1 * [E11,E12,E13,E14,E15],\n",
    "    S2 * [E21,E22,E23,E24,E25],\n",
    "    S3 * [E31,E32,E33,E34,E35],\n",
    "]\n",
    "```\n",
    "\n",
    "# 在解码输出阶段注意力范围应该是全部输入隐藏状态吗？\n",
    "\n",
    "如果是序列到序列模式，两个 rnn 神经元接收到的输入文本不同，经过各自的 embedding 再建立注意力的话就是**交叉注意力**\n",
    "\n",
    "但我们的实现是解码器的输入文本是是编码器的输入文本生成的，实际上应该算是**双向自注意力**，因为解码器输入的注意范围是全部输入\n",
    "\n",
    "但也许应该只让解码器注意当前输入之前的文本，因为 大 名 鼎 鼎 的 Transformer 模型架构 就是这么做的，这种形式叫**单向自注意力**\n",
    "\n",
    "下面是一段简单的交叉注意力编解码器模型实现。\n",
    "\n",
    "- *由于注意力机制导致编解码器的耦合程度变大，在下面的实现中我们会将模型组件放进同一个模型里以便更好展示。*\n",
    "\n",
    "- *若使用 nn.LSTM 依照经典编解码器模型实现注意力机制的方法，应注意 outputs 是多层LSTM最后一层每步的隐藏状态, hiddens 是多层LSTM最后一步的每层的因此隐藏状态。在引入注意力机制时应该手动循环解码器的每一步并记录隐藏状态，并且让编码器每层注意相应层次的隐藏状态*\n",
    "\n",
    "- *在下面的实现中若解码器的词嵌入维度等于隐藏层维度，那么可以使解码器的 rnn 输入维度等于隐藏层维度从而去掉投影层*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9a29faa-b9e1-43ca-83eb-09ee59c1c899",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionEDM(torch.nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_vs: int,\n",
    "        output_vs: int,\n",
    "        emb_size: int,\n",
    "        hidden_size: int,\n",
    "        num_layers: int,\n",
    "        dropout: float = 0,\n",
    "        teacher_forcing_ratio: float = 0.5,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # 如果两个序列来自同一文本则只需要一个共用的 Embedding\n",
    "        self.encoder_emb = torch.nn.Embedding(input_vs, emb_size)\n",
    "        self.encoder_lstm = torch.nn.LSTM(emb_size, hidden_size, num_layers, dropout=dropout, batch_first=True)\n",
    "        # 引入注意力机制需要使解码器词嵌入维度和隐藏层维度相同，或解码器词向量通过投影层转换成隐藏层维度\n",
    "        self.decoder_emb = torch.nn.Embedding(output_vs, emb_size)\n",
    "        self.projection = torch.nn.Linear(emb_size, hidden_size)\n",
    "        self.decoder_lstm = torch.nn.LSTM(emb_size, hidden_size, num_layers, dropout=dropout, batch_first=True)\n",
    "        self.lm = torch.nn.Linear(hidden_size, output_vs)\n",
    "        self.num_layers = num_layers\n",
    "        self.teacher_forcing_ratio = torch.tensor(teacher_forcing_ratio)\n",
    "        # # 如使用单向注意力则需要用 register_buffer 注册一个不会更新的下三角矩阵\n",
    "        # tril = torch.tril(torch.ones(4096, 4096))\n",
    "        # self.register_buffer(\"tril\", tril)\n",
    "\n",
    "    def forward(self, source: torch.Tensor, target: torch.Tensor):\n",
    "        embed_source: torch.Tensor = self.encoder_emb(source)\n",
    "        encoder_hiddens = []\n",
    "        state = None\n",
    "        for x in embed_source.unbind(1):\n",
    "            _, (hidden, cell) = self.encoder_lstm(x.unsqueeze(1), state)\n",
    "            state = (hidden, cell)\n",
    "            encoder_hiddens.append(hidden)\n",
    "        encoded = torch.stack(encoder_hiddens, dim=-2)\n",
    "        encoded_trans = torch.transpose(encoded, -2, -1)\n",
    "        sqrtT: float = encoded_trans.size(-1) ** 0.5\n",
    "        embed_target: torch.Tensor = self.decoder_emb(target)\n",
    "        x = embed_target[:, 0].unsqueeze(1)\n",
    "        outputs = []\n",
    "        for time_step, target_t in enumerate(embed_target.unbind(1)):\n",
    "            scores = self.projection(x).unsqueeze(0) @ encoded_trans\n",
    "            scores = torch.softmax(scores / sqrtT, dim=-1)\n",
    "            # # 单向注意力下会使用下三角矩阵将当前时间步之后的权重分数置零\n",
    "            # scores = scores.masked_fill(self.tril[:time_step, :time_step] == 0, float(\"-inf\"))\n",
    "            hidden = scores @ encoded\n",
    "            output, (hidden, cell) = self.decoder_lstm(x, (hidden.squeeze(-2), cell))\n",
    "            output = self.lm(output)\n",
    "            outputs.append(output)\n",
    "            if torch.rand(1) < self.teacher_forcing_ratio:\n",
    "                x = target_t.unsqueeze(1)\n",
    "            else:\n",
    "                x = self.decoder_emb(torch.argmax(output, dim=-1))\n",
    "        return torch.cat(outputs, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5d5ed76-f2aa-443f-9100-a5c7593938b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 15, 200])\n"
     ]
    }
   ],
   "source": [
    "model = AttentionEDM(100, 200, 50, 64, 2, 0.1)\n",
    "source = torch.randint(0, 100, (32, 10))\n",
    "target = torch.randint(0, 200, (32, 15))\n",
    "\n",
    "outputs = model(source, target)\n",
    "\n",
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b38643-ac49-4d33-adc7-fe036e8a877a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
