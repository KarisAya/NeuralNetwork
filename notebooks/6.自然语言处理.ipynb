{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df3a1b24-8925-4451-ac7d-e3884444b980",
   "metadata": {},
   "source": [
    "# 自然语言处理 NLP\n",
    "\n",
    "自然语言是人类思维的载体，但其非结构化、歧义性、多样性给机器理解带来巨大挑战，如：\n",
    "\n",
    "词与义的割裂：同一单词在不同语境中含义不同（如“Apple”指水果或公司）。\n",
    "\n",
    "形态复杂性：中文无空格分隔、德语复合词、英语时态变化等。\n",
    "\n",
    "长程依赖：句子语义可能依赖远处词汇（如“I **is** the 9th letter on the vocabulary list”）。\n",
    "\n",
    "解决路径：将语言转化为机器可处理的数值形式，而**分词 Tokenization**是这一过程的第一步。\n",
    "\n",
    "# 语言编码与分词基础\n",
    "\n",
    "1. 语言编码的三层抽象\n",
    "\n",
    "|层级|描述|示例|\n",
    "|----|----|----|\n",
    "|字符级|处理单个字母/符号|\"A\" → 65 (ASCII)|\n",
    "|子词级|平衡语义与效率（主流方案）|\"unhappy\" → [\"un\",\"happy\"]|\n",
    "|词级|整词为单位（简单但数量极大）|\"cat\" → 3047 (词表ID)|\n",
    "\n",
    "2. 分词器（Tokenizer）的核心作用\n",
    "   \n",
    "    文本 → 令牌（Token）序列：将句子拆分为模型可处理的单元\n",
    "    \n",
    "    \"I love NLP\" → [\"I\", \"love\", \"NLP\"]\n",
    "    \n",
    "    解决未登录词（OOV）问题：通过子词拆分覆盖生僻词\n",
    "\n",
    "    \"pneumonoultramicroscopicsilicovolcanoconiosis\" → [\"pneumono\", \"ultra\", \"micro\", \"scopic\", \"silico\", \"volcano\", \"coniosis\"]\n",
    "\n",
    "# 分词器的设计思路\n",
    "\n",
    "1. 基于规则的分词\n",
    "\n",
    "    依赖预定义规则，如空格、标点切分。\n",
    "\n",
    "2. 基于词频统计的分词\n",
    "\n",
    "    如 Byte Pair Encoding (BPE)（GPT/BERT所用方案）\n",
    "    \n",
    "    其核心思想是初始将文本拆分为单字符，然后基于频次等算法等策略合并为多字符加入词典。\n",
    "\n",
    "    直到词典规模达到预设值。\n",
    "\n",
    "    *如果基于频次合并则是 BPE 如果基于 频次/子词频次乘积则是 WordPiece，每种策略各有优点和不足。*\n",
    "\n",
    "4. 基于深度学习的分词\n",
    "\n",
    "    使用BiLSTM、Transformer等模型\n",
    "\n",
    "    将分词视为序列标注问题\n",
    "\n",
    "下面会实现使用 BPE 算法训练一个词汇表，训练文本使用 sklearn.datasets.fetch_20newsgroups，分词前先进行了去除空白字符和按空格预分词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03c064b6-cd78-4e31-8cd1-9448da0604aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[UNK]', '[PAD]', '[BOS]', '[EOS]', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', ' ', 'kar']\n",
      "[37, 8, 15, 15, 18, 56, 57, 12, 22, 0, 0, 0]\n",
      "H_e_l_l_o_ _kar_i_s_[UNK]_[UNK]_[UNK]\n"
     ]
    }
   ],
   "source": [
    "# 这里会实现一个最简单的语言编解码器，字典只有字母,空格还有测试字符对 'kar'\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, data: list[str]) -> None:\n",
    "        self.ind2char = [\"[UNK]\", \"[PAD]\", \"[BOS]\", \"[EOS]\"]\n",
    "        self.ind2char.extend(data)\n",
    "        self.single_token_length = sorted(set(len(c) for c in self.ind2char), reverse=True)\n",
    "        self.char2ind = {c: i for i, c in enumerate(self.ind2char)}\n",
    "\n",
    "    def encode(self, text: str) -> list[int]:\n",
    "        tokens: list[int] = []\n",
    "        while len(text) > 0:\n",
    "            for length in self.single_token_length:\n",
    "                if length > len(text):\n",
    "                    continue\n",
    "                if token := self.char2ind.get(text[:length]):\n",
    "                    tokens.append(token)\n",
    "                    text = text[length:]\n",
    "                    break\n",
    "            else:\n",
    "                text = text[length:]\n",
    "                tokens.append(self.char2ind[\"[UNK]\"])\n",
    "        return tokens\n",
    "\n",
    "    def decode(self, indices: list[int], hyphen: str = \"\") -> str:\n",
    "        return hyphen.join(self.ind2char[i] for i in indices)\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(list(\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ \") + [\"kar\"])\n",
    "print(tokenizer.ind2char)\n",
    "encoded = tokenizer.encode(\"Hello karis!!!\")\n",
    "print(encoded)\n",
    "print(tokenizer.decode(encoded, hyphen=\"_\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d055a35-6bbd-4856-8e0a-26adc4ffc66e",
   "metadata": {},
   "source": [
    "# BPE 算法\n",
    "\n",
    "BPE 算法的目标是通过迭代合并高频符号对，逐步构建一个子词词汇表。其核心思想是：\n",
    "\n",
    "初始化：将词汇表初始化为所有字符。\n",
    "\n",
    "合并：找到频率最高的符号对，将其合并为一个新的子词，并更新词汇表。\n",
    "\n",
    "迭代：重复上述过程，直到词汇表达到预设大小或没有更多符号对可合并。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "243829a7-5b7c-4178-bc75-f9d1f6c1f0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import re\n",
    "\n",
    "\n",
    "def pre_tokenizers(train_data: list[str]):\n",
    "    whitespace = re.compile(r\"\\s+\")\n",
    "    new_train_data = []\n",
    "    for text in train_data:\n",
    "        new_train_data.extend(whitespace.sub(\" \", text).split())\n",
    "    return new_train_data\n",
    "\n",
    "\n",
    "train_data = pre_tokenizers(fetch_20newsgroups(subset=\"train\")[\"data\"][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45400873-1823-4d44-987f-d52150d6b619",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def train_bpe(texts, vocab_size):\n",
    "    # 初始化词汇表为字符,统计初始符号频率\n",
    "    symbols_list = [list(text) for text in texts]\n",
    "    vocab = set()\n",
    "    for symbols in symbols_list:\n",
    "        vocab.update(symbols)\n",
    "    len_vocab = len(vocab)\n",
    "    pairs: Counter[tuple[str, str]]\n",
    "    while len_vocab < vocab_size:\n",
    "        pairs = Counter()\n",
    "        for symbols in symbols_list:\n",
    "            for i in range(len(symbols) - 1):\n",
    "                pairs[(symbols[i], symbols[i + 1])] += 1\n",
    "        if not pairs:\n",
    "            break\n",
    "        bp_l, bp_r = max(pairs, key=lambda x: pairs[x])\n",
    "        best_pair = bp_l + bp_r\n",
    "        vocab.add(best_pair)\n",
    "        len_vocab += 1\n",
    "        # 更新文本中的该符号对\n",
    "        new_symbols_list = []\n",
    "        for symbols in symbols_list:\n",
    "            new_symbols = []\n",
    "            len_symbols = len(symbols) - 1\n",
    "            i = 0\n",
    "            while i < len_symbols:\n",
    "                if (symbol := symbols[i]) == bp_l and symbols[i + 1] == bp_r:\n",
    "                    new_symbols.append(best_pair)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_symbols.append(symbol)\n",
    "                    i += 1\n",
    "            else:\n",
    "                if i == len_symbols:\n",
    "                    new_symbols.append(symbols[i])\n",
    "            new_symbols_list.append(new_symbols)\n",
    "        symbols_list = new_symbols_list\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b692eda-6718-4f2f-bec0-859ba92a2194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分词耗时：14.378931045532227\n",
      "{'ill', 'Y', 'ul', 'ound', 'pl', 'sc', \"I'\", '$', 'tion', 'ganiz', '16', 'ence', 'sp', 'ain', 'ost', 'ell', 'P', 'ment', 'we', 'ber', 'cont', 'ast', 'Tur', 'ich', 'per', '(', 'ory', 'K', 'des', 'than', '====', 'Re:', 'w', 'pp', '!', 'when', 'not', 'ame', 'very', 'k', \"'s\", 'man', 'ers', 'ess', 'ri', '14', 'fore', 'lo', 'st', 'you', '!-*-', 'Z', 'ou', 'bu', '17', 'fin', 'ual', 'con', 'ont', 'r', 'only', 'es.', '19', 'them', 'ight', '25', 'ect', 'ks', 'T', 'ly', '__', 'ide', 'rom', 'ible', '%%%%', 'ans', 'Organization:', 'which', 'about', 'ld', 'y', '[', '========', '5', 'bl', 'ent', 'An', 'age', 'arm', 'op', 'ie', 'ro', 'ply', 'sy', 'ews', 'osting', 'az', '~', 'ec', 'x', 'ar', 'from', '--------------------------------', 'University', 'br', 'work', 'ause', 'am', '##', 'end', 'Armenian', 'In', 'X', 'ot', 'sa', 'ew', '20', '________', 'ition', 'vi', 'U', 's.', 'es:', 'ive', 'dri', 'C', 'ite', 'im', 'ong', 'af', 'will', 'may', ']', 'Lin', 'SCSI', 'ia', 'ce', 'icle', 'what', 'one', 'there', 'v', 'qu', 'og', 'sh', 'F', 'la', 'est', 'ic', 'ob', 'ose', 'writ', 'wh', 'po', 'a', 'ure', '12', 'tr', 'dec', 'for', 'l', '!-', 'f', 'Uni', 'ph', '.edu', 'now', 'ne', 'nt', 'our', 'Dis', 'ist', 'ap', 'writes:', 'ing', 'ry', '-P', \"on't\", 'reat', 'cr', 'acc', 'ub', 'ass', 'Ch', ')', 'ex', 'ate', '\"', 'can', 'ust', 'cl', 'I', 'ed.', 'TP', 'Armen', ':', 'der', 'ith', 'ents', '|', 'ix', 'ser', 'so', 'ey', 'The', 'how', 'c', 'et', 'J', 'no', 'and', 'by', 'on', 'ow', 'li', 'ous', 'ost:', '10', '`', 'pr', '----------------', 'S', 'ter', 'z', 'Al', 'the', 'en', 'ard', 'gu', 'bet', 'do', 'As', 'rit', 'over', 'able', '6', 'ther', 'Apr', 'are', '@c', 'c.', 'um', 'vers', '****', \"n't\", 'TI', 'rel', 'because', '--------', 'It', 'tw', 'ni', 'read', 'igh', 'itt', 'th', 'go', '_', 'le', 'let', 'od', 'av', '-Posting', 'te', 'ies', '*', 'm', '================================', 'g', 'know', 'like', ',', 'way', '^', 'call', 'a.', 'ac', 'ple', 'id', 'ft', 'or', 'ke', 'ss', 'port', 'Subject', 'less', 'gen', 'pre', 'H', 're', 'SI', 'all', '93', 'ip', 'stand', 'put', 'is', 'ant', 'd', '.\"', 'G', '-', 'their', 'would', 'many', '3', 'This', 'min', 's', 'if', 'i', 'ay', '>', 'say', 'were', '9', 'an', 'ad', 'ough', 'q', 'j', 'ur', 'E', 'where', 'ical', '-Posting-Host:', \"I'm\", 'ff', 'ally', 'dow', '-H', 'b', 'ail', 'inter', 'tim', 'think', 'had', 'thing', 'Lines:', 'ble', 'ink', '.com', 'ire', 'tribut', 'gh', 'N', 'of', 'ation:', 'om', 'use', 'be', 'L', 'res', 'art', 'de', '..', 'into', 'ine', ';', 'ha', 'ed', 'Re', 'day', 'ak', 'R', 'es', 'have', 'SA', 'dis', '+', '____', 'ress', 'ech', '000', '>>>', 'out', 'fer', '24', 'Subject:', 'n', 'they', 'could', 'O', 'V', '^^^^', 'cur', 'ity', '\\\\', 'sion', 'ed,', 'To', 'St', 'ary', 'iv', 'low', '15', 'just', \"'t\", 'ab', 'lin', 'ach', 'as', 'people', 'your', 'SC', \"'\", 'ice', 'he', 'NN', 'in', 'ig', 'ch', 'From:', 'act', 'tri', '2', 'auto', 'Russian', '1', 'us', 'men', 'tern', 'urn', 'str', 'su', 'ever', '{', '/', 'does', '&', '.', 'int', '1993', 'au', 'h', '}', 'nd', 't', 'with', 'ir', \"don't\", 'gan', 'ck', '@', 'el', 'at', 'been', 'Th', 'ance', 'att', 'my', 'co', 'that', 'il', 'ere', 'ra', 'pro', '7', 'From', '0', 'peop', 'who', 'up', '8', 'under', '--', 'Com', 'article', '!-*', '4', 'y,', 'its', 'versity', 'tribution', '%%', 'ct', 'other', 'ation', 'form', 'ort', 'ile', 'it', 'If', 'ok', 'er', '.1', 'iz', 'ear', 'has', 'ick', 'com', 'ge', 'p', 'em', ').', 'A', 'Or', '**', 'B', 'fo', '########', 'ci', 'edu', 'ign', 'D', 's,', 'W', 'Q', 'ag', 'cause', 'gr', 'stem', 'al', 'bo', 'Rus', 'Russ', '00', 'o', 'uc', 'off', 'ree', '<', '==', 'Organiz', 'was', 'wor', '********', 'd.', 'ian', 'bit', 'ace', 'pt', 'ut', 'Do', 'ish', '=', 'ase', 'ol', 'u', '#', 'll', '================', 'e,', 've', 'se', 'ould', 'ven', 'this', 'mo', 'ver', 'year', '...', 'ack', 'ome', 'es,', 'ind', '####', '?', 'S.', 'me', 'ion', 'ject', 'Sub', '-Posting-H', 'car', 'M', 'un', 'sur', 'comp', 'ud', '.c', 'clo', 'du', 'to', '^^', 'ations', 'old', 'ma', 'e', 'ful', '%', 'more', 'ated', 'new', 'any', 'ang', '----', 'ility', '>>', '!-*-!-*-', 'Wh', 'vol', 'Ar', 'e.', 'No', '....', 'get', 'but', 'pe', 'some'} 600\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "vocab = train_bpe(train_data, 600)\n",
    "end = time.time()\n",
    "print(f\"分词耗时：{end - start}\")\n",
    "print(vocab, len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd36abe1-e43a-4b0d-a659-79b213655f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分词耗时：0.09062838554382324\n",
      "{'ill', 'ound', 'ul', 'Y', 'pl', 'sc', 'ject:', \"I'\", '$', 'tion', '16', 'ence', 'sp', 'ain', 'ost', 'ell', 'P', 'ment', 'we', 'ber', 'cont', 'ast', 'Tur', 'ich', 'per', '(', 'ory', 'K', 'des', 'than', '====', 'Re:', 'w', 'pp', 'when', '!', 'ame', 'not', 'very', 'k', \"'s\", 'man', 'ers', 'ization:', 'ess', 'ri', '14', 'fore', 'lo', 'st', 'you', '!-*-', 'Z', 'ou', 'bu', '17', 'fin', 'ual', 'con', 'ont', 'r', 'only', 'es.', '19', 'them', '25', 'ight', 'ect', 'ks', 'ly', 'T', '__', 'ide', 'rom', 'ible', '%%%%', 'ans', 'which', 'Organization:', 'about', 'ld', 'y', '[', '========', '5', 'bl', 'An', 'ent', 'arm', 'age', 'op', 'ie', 'ro', 'ply', 'sy', 'ews', 'osting', 'az', '~', 'ec', 'x', 'ar', 'from', '--------------------------------', 'University', 'br', 'work', 'ause', 'am', '##', 'end', 'Armenian', 'In', 'X', 'ot', 'ew', '20', '________', 'ople', 'ition', 'vi', 'U', 's.', 'es:', 'ive', 'dri', 'C', 'ite', 'im', 'ong', 'Mar', 'af', 'will', 'may', ']', 'Lin', 'SCSI', 'ia', 'ce', 'icle', 'what', 'there', 'one', 'v', 'og', 'qu', 'sh', 'F', 'la', 'est', 'ic', 'ob', 'ose', 'writ', 'wh', 'po', 'ure', '12', 'a', 'tr', 'dec', 'for', 'l', '!-', 'f', 'Uni', 'ph', '.edu', 'now', 'ne', 'our', 'Dis', 'nt', 'ist', 'ap', 'writes:', 'ing', 'ry', 'osting-H', '-P', \"on't\", 'reat', 'cr', 'ub', 'ass', 'Ch', ')', 'ate', 'ex', '\"', 'can', 'ust', 'cl', 'I', 'ed.', 'TP', 'Armen', ':', 'der', 'ith', 'ents', '|', 'ix', 'ser', 'so', 'ey', 'The', 'how', 'c', 'et', 'J', 'no', 'and', 'by', 'on', 'ost:', 'ous', 'ow', 'li', '10', 'pr', '`', '----------------', 'S', 'ter', 'z', 'Al', 'the', 'en', 'ard', 'gu', 'bet', 'As', 'do', 'rit', 'over', 'able', '6', 'ther', 'Apr', 'are', '@c', 'c.', 'um', 'rel', '****', \"n't\", 'TI', 'vers', 'because', '--------', 'tw', 'It', 'ni', 'read', 'igh', 'itt', 'th', 'go', '_', 'le', 'let', 'od', 'av', 'mis', 'te', 'ies', '*', 'm', '================================', 'g', 'know', 'like', ',', 'way', '^', 'call', 'a.', 'ac', 'ft', 'id', 'ple', 'or', 'ke', 'ss', 'port', 'less', 'gen', 'pre', 'H', 're', 'SI', 'all', '93', 'ip', 'stand', 'put', 'is', 'ant', '.\"', 'd', 'G', '-', 'their', 'would', 'many', '3', 'min', 's', 'if', 'ay', 'i', '>', 'say', 'were', 'ad', '9', 'an', 'ough', 'q', 'j', 'ur', 'where', 'E', 'ical', '-Posting-Host:', \"I'm\", 'ff', 'ally', 'dow', '-H', 'cess', 'b', 'ail', 'inter', 'tim', 'think', 'had', 'thing', 'Lines:', 'ble', 'ink', '.com', 'ire', 'tribut', 'gh', 'N', 'of', 'om', 'use', 'be', 'L', 'res', 'art', 'de', '..', 'into', 'ine', ';', 'ha', 'ed', 'Re', 'day', 'ak', 'R', 'es', 'SA', 'have', 'dis', '+', '____', 'ress', 'ech', '>>>', '000', 'out', 'fer', '24', 'Subject:', 'n', 'they', 'could', 'O', 'V', '^^^^', 'cur', 'ity', '\\\\', 'sion', 'ed,', 'To', 'St', 'ary', 'iv', 'low', 'just', '15', \"'t\", 'ab', 'lin', 'ach', 'as', 'people', 'your', 'SC', \"'\", 'ice', 'he', 'NN', 'ch', 'in', 'ig', 'From:', 'act', 'tri', '2', 'auto', 'Russian', 'us', 'men', 'tern', '1', 'urn', 'str', 'ever', 'su', '{', '/9', '/', 'does', '&', '.', 'int', '1993', 'au', 'h', '}', 'nd', 'ir', 'with', \"don't\", 't', 'gan', 'ck', '@', 'el', 'at', 'been', 'Th', 'att', 'ance', 'my', 'co', 'that', 'il', 'ere', 'ra', 'pro', '7', 'From', '0', 'who', 'up', '8', 'under', '--', 'Com', 'article', '!-*', '4', 'y,', 'its', 'versity', 'ct', '%%', 'other', 'ation', 'form', 'ort', 'ile', 'it', 'If', 'ok', 'er', '.1', 'iz', 'ear', 'has', 'ick', 'com', 'ge', ').', 'em', 'p', 'A', 'Or', '**', '########', 'fo', 'B', 'ci', 'edu', 'ign', 'D', 's,', 'Organ', 'Q', 'W', 'ag', 'cause', 'stem', 'gr', 'Rus', 'al', 'bo', 'Russ', '00', 'o', 'uc', 'off', 'ree', '<', '==', 'was', 'wor', 'd.', '********', 'ian', 'bit', 'pt', 'ut', 'Do', 'ish', '=', 'ase', 'ol', 'u', '#', 'll', '================', 'e,', 've', 'se', 'ould', 'ven', 'this', 'mo', 'ver', 'year', '...', 'ack', 'ome', 'es,', 'ind', '####', '?', 'S.', 'me', 'ject', 'ion', 'Sub', '-Posting-H', 'ization', 'car', 'M', 'sur', 'un', 'comp', 'ud', 'pe', '.c', 'clo', 'du', 'to', 'ations', '^^', 'old', 'ma', 'e', 'ful', '%', 'more', 'ated', 'ang', 'any', 'ility', '----', 'new', '>>', '!-*-!-*-', 'Wh', 'vol', 'Ar', 'e.', 'No', '....', 'get', 'but', 'ace', 'some'} 600\n"
     ]
    }
   ],
   "source": [
    "# 使用现有的第三方实现\n",
    "\n",
    "from tokenizers import models, Tokenizer, trainers\n",
    "\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "start = time.time()\n",
    "tokenizer.train_from_iterator(\n",
    "    train_data,\n",
    "    trainer=trainers.BpeTrainer(vocab_size=600),\n",
    ")\n",
    "std_vocab = set(tokenizer.get_vocab().keys())\n",
    "end = time.time()\n",
    "print(f\"分词耗时：{end - start}\")\n",
    "print(std_vocab, len(std_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "864e6a61-8609-436a-a43d-45413fb2a688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'peop', 'This', 'ation:', 'acc', 'sa', 'Organiz', 'tribution', '-Posting', 'ganiz', 'Subject'}\n",
      "{'ople', 'ization:', '/9', 'Mar', 'ject:', 'Organ', 'ization', 'cess', 'mis', 'osting-H'}\n"
     ]
    }
   ],
   "source": [
    "# 查看分词集合差异\n",
    "print(vocab.difference(std_vocab))\n",
    "print(std_vocab.difference(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ccdd0e-605c-400d-a632-843653707126",
   "metadata": {},
   "source": [
    "可以发现我们自己的纯 Python 实现和 tokenizers 库的计算结果相差不大（600词汇相差10个词），可能是在统计最高频词时对同频次词汇处理方式不一致导致的差异。\n",
    "\n",
    "但不管怎么说，tokenizers 库实现的速度要快的多。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b3c1a6-c49f-48e1-82b5-8e2e21aee01a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "460bd11a-a94c-4562-9ae6-f9a4785a42f7",
   "metadata": {},
   "source": [
    "# 基于深度学习的分词\n",
    "\n",
    "基于深度学习的分词方法将分词任务建模为序列标注或序列生成问题，利用神经网络自动学习文本特征和分词规律。以下是其核心原理：\n",
    "\n",
    "1. 问题建模方式\n",
    "\n",
    "    (1) 序列标注方法（主流方法）\n",
    "    \n",
    "    将分词转化为字符级别的分类任务，常用标签体系：\n",
    "    \n",
    "    B/I/E/S：Begin(词首)/Inside(词中)/End(词尾)/Single(单字词)\n",
    "    \n",
    "    B/M/E/W/O：Begin/Middle/End/Whole(整词)/Outside(非词)\n",
    "    ```\n",
    "    示例：\n",
    "    原句：深度学习很强大\n",
    "    标注：B E B E S B E\n",
    "    分词：深度/学习/很/强大\n",
    "    ```\n",
    "\n",
    "   (2) 序列生成方法\n",
    "   \n",
    "    将分词视为从字符序列到词序列的转换问题，使用Encoder-Decoder架构生成分词结果。\n",
    "\n",
    "2. 关键技术特点\n",
    "\n",
    "    上下文感知：\n",
    "    \n",
    "    能解决歧义切分问题 \"下雨天留客天留我不留\"\n",
    "    \n",
    "    自动识别未登录词\n",
    "\n",
    "    端到端训练：\n",
    "    \n",
    "    无需人工设计特征\n",
    "    \n",
    "    自动学习有效的文本表示\n",
    "    \n",
    "    迁移学习：\n",
    "    \n",
    "    可利用预训练语言模型(如BERT)提升性能\n",
    "    \n",
    "    在小样本数据上表现良好\n",
    "\n",
    "3. 优势与局限\n",
    "\n",
    "    优势：\n",
    "    \n",
    "    自动学习复杂特征\n",
    "    \n",
    "    对未登录词自适应强\n",
    "    \n",
    "    多语言适应性强\n",
    "    \n",
    "    局限：\n",
    "    \n",
    "    需要大量标注数据\n",
    "    \n",
    "    模型可解释性差\n",
    "    \n",
    "    计算资源消耗大\n",
    "\n",
    "*在本项目的代码库中实现了一个简单的使用深度学习训练的分词器，在此处不进行展开。*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4cd1aa-3ac6-4a5c-a577-5d1af875250a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
