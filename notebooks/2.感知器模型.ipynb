{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "356a6f36-c0d7-4837-818f-ef21556608e4",
   "metadata": {},
   "source": [
    "# 感知器模型 Perceptron\n",
    "\n",
    "感知器是1957年由Frank Rosenblatt 提出的一种单层人工神经网络模型，被视为神经网络与机器学习领域的奠基性工作之一。\n",
    "\n",
    "其本质是一种线性二分类器，通过调整权重对输入数据进行类别划分，为后续多层神经网络（如多层感知机MLP）的发展奠定了基础。\n",
    "\n",
    "下面是经典4×4感知器模型区分字母T,J的例子\n",
    "\n",
    "```python\n",
    "T = [\n",
    "    [1,1,1,0,\n",
    "     0,1,0,0,\n",
    "     0,1,0,0,\n",
    "     0,1,0,0,],\n",
    "    \n",
    "    [0,1,1,1,\n",
    "     0,0,1,0,\n",
    "     0,0,1,0,\n",
    "     0,0,1,0,],\n",
    "]\n",
    "J = [\n",
    "    [0,0,1,0,\n",
    "     0,0,1,0,\n",
    "     1,0,1,0,\n",
    "     1,1,1,0,],\n",
    "    \n",
    "    [0,0,0,1,\n",
    "     0,0,0,1,\n",
    "     0,1,0,1,\n",
    "     0,1,1,1,],\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81773385-458f-4e12-ba26-049e8336074c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练前\n",
      "T: [0.0, 0.0]\n",
      "J: [0.0, 0.0]\n",
      "训练后\n",
      "T: [-1.0000003207718833, -0.9999967553958412]\n",
      "J: [0.9999972307565017, 0.9999997979499405]\n"
     ]
    }
   ],
   "source": [
    "T = [[1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0], [0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0]]\n",
    "J = [[0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0], [0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1]]\n",
    "\n",
    "# 权重层\n",
    "w = [0.0 for _ in range(16)]\n",
    "# 截距项\n",
    "b = 0.0\n",
    "\n",
    "\n",
    "# 输出\n",
    "def perceptron_output(inputs, w, b):\n",
    "    z = 0\n",
    "    for i in range(16):\n",
    "        z += inputs[i] * w[i] + b\n",
    "    return z\n",
    "\n",
    "\n",
    "print(\"训练前\")\n",
    "print(\"T:\", [perceptron_output(t, w, b) for t in T])\n",
    "print(\"J:\", [perceptron_output(j, w, b) for j in J])\n",
    "\n",
    "# 训练参数\n",
    "eval_iters = 200\n",
    "learning_rate = 0.1\n",
    "\n",
    "# 首先 定义 T 输出值 -1 ,J 输出值 1\n",
    "\n",
    "T_OUT = -1\n",
    "J_OUT = 1\n",
    "\n",
    "# 执行梯度下降\n",
    "for _ in range(eval_iters):\n",
    "    # 预测值\n",
    "    dw_list = []\n",
    "    db_list = []\n",
    "    pred = perceptron_output(T[0], w, b)\n",
    "    dw_list.append([(pred - T_OUT) * i for i in T[0]])\n",
    "    db_list.append(pred - T_OUT)\n",
    "    pred = perceptron_output(T[1], w, b)\n",
    "    dw_list.append([(pred - T_OUT) * i for i in T[1]])\n",
    "    db_list.append(pred - T_OUT)\n",
    "    pred = perceptron_output(J[0], w, b)\n",
    "    dw_list.append([(pred - J_OUT) * i for i in J[0]])\n",
    "    db_list.append(pred - J_OUT)\n",
    "    pred = perceptron_output(J[1], w, b)\n",
    "    dw_list.append([(pred - J_OUT) * i for i in J[1]])\n",
    "    db_list.append(pred - J_OUT)\n",
    "    dw = []\n",
    "    for i in range(16):\n",
    "        dw.append(sum(dw_list[j][i] for j in range(4)) / 4)\n",
    "    db = sum(db_list) / 4\n",
    "    w = [w[i] - learning_rate * dw[i] for i in range(16)]\n",
    "    b -= learning_rate * db\n",
    "\n",
    "# 训练结果\n",
    "print(\"训练后\")\n",
    "print(\"T:\", [perceptron_output(t, w, b) for t in T])\n",
    "print(\"J:\", [perceptron_output(j, w, b) for j in J])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a395dd-e702-46ed-b306-e2dee8d6de3d",
   "metadata": {},
   "source": [
    "可以看到我们之前的定义 T 输出值 < 0 ,J 输出值 > 0 \n",
    "\n",
    "经过 200 次梯度下降训练之后 输入 T 的值几乎为 -1，而 J 的值几乎为 1 \n",
    "\n",
    "可以说训练成功"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8122d0e1-a30e-4e2d-b2ff-de631fda6742",
   "metadata": {},
   "source": [
    "# 张量 Tensor\n",
    "\n",
    "张量是多维数组的泛化形式，用于表示高维空间中的线性关系数据。其阶数（Rank）定义了维度层级，是深度学习框架（如TensorFlow、PyTorch）的核心数据结构。\n",
    "\n",
    "我们发现，在实现感知器模型代码里，存在一些不足之处，例如：\n",
    "\n",
    "- 对不同的输入和输出，单独处理，导致代码量增多\n",
    "- 不同的输入输出互不依赖可并行计算，但额外实现并行会变得非常复杂\n",
    "- 计算梯度的候每次更新权重总要把权重和输入显式遍历。\n",
    "\n",
    "上述问题我们均可以使用张量运算进行简化表示。\n",
    "\n",
    "1. 把不同的输入和输出压合并在一起\n",
    "2. 输入和权重行列线性运算时可直接表示为矩阵乘法\n",
    "3. 求梯度时直接进行张量点乘批量运算\n",
    "\n",
    "上面代码直接表示为如下形式，变得十分简洁。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a28ba148-7c80-41d8-a37c-daff4aa438ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练前\n",
      "tensor([[0., 0.],\n",
      "        [0., 0.]])\n",
      "训练后\n",
      "tensor([[-1.0000, -1.0000],\n",
      "        [ 1.0000,  1.0000]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "T = [[1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0], [0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0]]\n",
    "J = [[0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0], [0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1]]\n",
    "inputs = torch.stack((torch.tensor(T, dtype=torch.float), torch.tensor(J, dtype=torch.float)))\n",
    "outputs = torch.tensor([[-1, -1], [1, 1]], dtype=torch.float)\n",
    "# 权重层\n",
    "w = torch.zeros(16)\n",
    "# 截距项\n",
    "b = 0.0\n",
    "\n",
    "\n",
    "# 输出\n",
    "def perceptron_output(inputs, w, b):\n",
    "    return inputs @ w + b\n",
    "\n",
    "print(\"训练前\")\n",
    "print(perceptron_output(inputs, w, b))\n",
    "\n",
    "# 训练参数\n",
    "eval_iters = 200\n",
    "learning_rate = 0.1\n",
    "\n",
    "# 执行梯度下降\n",
    "for _ in range(eval_iters):\n",
    "    pred = perceptron_output(inputs, w, b)\n",
    "    err = pred - outputs\n",
    "    b -= learning_rate * err.mean()\n",
    "    dw = err.unsqueeze(-1) * inputs\n",
    "    w -= learning_rate * dw.mean(dim=[0, 1])\n",
    "\n",
    "# 训练结果\n",
    "print(\"训练后\")\n",
    "print(perceptron_output(inputs, w, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dbcaf1-26b7-40af-94e2-a98ca3f48fc3",
   "metadata": {},
   "source": [
    "# 关于 PyTorch\n",
    "\n",
    "上面我们清晰感受到了 PyTorch 诸如之类的深度学习框架对模型实现的简化能力。\n",
    "\n",
    "PyTorch 作为当前主流的深度学习框架之一，其动态计算图机制和自动微分系统彻底改变了传统梯度计算的实现方式。\n",
    "\n",
    "在标准PyTorch训练中，开发者只需关注三个核心要素：\n",
    "\n",
    "1. 张量运算：通过torch.Tensor实现数据的高效存储与并行计算\n",
    "2. 自动微分：利用autograd模块自动追踪运算图的梯度信息\n",
    "3. 优化器体系：通过torch.optim提供SGD/Adam等优化算法的标准化实现\n",
    "\n",
    "下面是使用 PyTorch 实现感知器模型范例\n",
    "\n",
    "下面的代码实际上是 PyTorch 的范式，我们在代码内每步都进行了注释"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da636c72-fbc0-4cd3-bc52-827e53b56def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练前\n",
      "tensor([[[0.5146],\n",
      "         [0.2753]],\n",
      "\n",
      "        [[0.5183],\n",
      "         [0.3044]]], grad_fn=<ViewBackward0>)\n",
      "训练后\n",
      "tensor([[[-1.0000],\n",
      "         [-1.0000]],\n",
      "\n",
      "        [[ 1.0000],\n",
      "         [ 1.0000]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "T = [[1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0], [0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0]]\n",
    "J = [[0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0], [0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1]]\n",
    "import torch\n",
    "\n",
    "inputs = torch.stack((torch.tensor(T, dtype=torch.float), torch.tensor(J, dtype=torch.float)))\n",
    "outputs = torch.tensor([[-1, -1], [1, 1]], dtype=torch.float)\n",
    "\n",
    "# 训练参数\n",
    "eval_iters = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "model = torch.nn.Linear(16, 1) # 感知器模型本质上就是一个线性层\n",
    "\n",
    "print(\"训练前\")\n",
    "print(model(inputs))\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 执行梯度下降\n",
    "for _ in range(eval_iters):\n",
    "    pred = model(inputs).squeeze(-1) # 线性层输出张量大小指定为 1 意味着输出为形状为 [1] 的向量而不是标量，这里做一下调整，对计算没有影响\n",
    "    loss = torch.nn.functional.mse_loss(pred, outputs) # 用一种合适的方式计算梯度下降方向，这里用的标准差，但梯度下降方向之前实现的均差方向一致\n",
    "    optimizer.zero_grad() # 清除算法梯度\n",
    "    loss.backward() # 损失向后传播\n",
    "    optimizer.step() # 更新模型（线性层）参数\n",
    "    \n",
    "# 训练结果\n",
    "\n",
    "print(\"训练后\")\n",
    "print(model(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4aa159e-1c32-4341-8b79-d04936efae0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
