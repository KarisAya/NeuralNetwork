{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b4130ce-95f6-4609-952e-660fda9f6e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85ff4282-ad20-41ff-a495-532934aecab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载训练数据\n",
    "def load_words(file_path: str | Path):\n",
    "    with open(file_path, \"r\", encoding=\"utf8\") as f:\n",
    "        lines = f.readlines()\n",
    "        return [word for line in lines if (word := line.rstrip(\"\\n\").split())]\n",
    "\n",
    "\n",
    "train_words = load_words(\"pku_training.utf8\")\n",
    "gold_words = load_words(\"pku_test_gold.utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52c973b4-1789-4502-9591-0e8f303a0969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 进行 BIEOS 标注\n",
    "def bieos_tag(vocab: set[str]):\n",
    "    range_chinese = range(ord(\"\\u4e00\"), ord(\"\\u9fff\") + 1)\n",
    "    range_lower_eng = range(ord(\"a\"), ord(\"z\") + 1)\n",
    "    range_upper_eng = range(ord(\"A\"), ord(\"Z\") + 1)\n",
    "    range_number = range(ord(\"0\"), ord(\"9\") + 1)\n",
    "\n",
    "    def is_character(order: int):\n",
    "        return order in range_chinese or order in range_lower_eng or order in range_upper_eng or order in range_number\n",
    "\n",
    "    bieos_dataset = {}\n",
    "\n",
    "    for word in vocab:\n",
    "        lst = []\n",
    "        l = 0\n",
    "        for char in word:\n",
    "            l += 1\n",
    "            if is_character(ord(char)):\n",
    "                lst.append(\"I\")\n",
    "            else:\n",
    "                lst.append(\"O\")\n",
    "        for i in range(l):\n",
    "            if lst[i] == \"I\":\n",
    "                lst[i] = \"B\"\n",
    "                break\n",
    "        for i in range(l - 1, -1, -1):\n",
    "            if lst[i] == \"I\":\n",
    "                lst[i] = \"E\"\n",
    "                break\n",
    "        else:\n",
    "            for i in range(l):\n",
    "                if lst[i] == \"B\":\n",
    "                    lst[i] = \"S\"\n",
    "                    break\n",
    "        bieos_dataset[word] = lst\n",
    "    return bieos_dataset\n",
    "\n",
    "\n",
    "bieos_dataset = bieos_tag(set(word for sent in train_words for word in sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9631a75-5af5-4f13-8460-40dd34520f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建分词器\n",
    "class Tokenizer:\n",
    "    def __init__(self, bieos_dataset: dict[str, list[str]], vocab: list[str] | None = None) -> None:\n",
    "        self.bieos_dataset = bieos_dataset\n",
    "        self.id2tag = [\"<STOP>\", \"B\", \"I\", \"O\", \"E\", \"S\", \"<START>\"]\n",
    "        self.tag2id = {t: i for i, t in enumerate(self.id2tag)}\n",
    "        if vocab is not None:\n",
    "            self.id2token = vocab\n",
    "        else:\n",
    "            vocab_set = set()\n",
    "            for word in bieos_dataset.keys():\n",
    "                vocab_set.update(set(word))\n",
    "                # vocab_set.add(word)\n",
    "            self.id2token = sorted(vocab_set, key=lambda x: ord(x))\n",
    "            self.id2token = [\"[PAD]\", \"[UNK]\", \"[BOS]\", \"[EOS]\"] + self.id2token\n",
    "\n",
    "        self.token2id = {c: i for i, c in enumerate(self.id2token)}\n",
    "\n",
    "    def encode(self, text: str) -> list[int]:\n",
    "        return [self.token2id.get(c, 1) for c in text]\n",
    "\n",
    "    def decode(self, indices: list[int]) -> str:\n",
    "        return \"\".join(self.id2token[i] for i in indices)\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(bieos_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9085a251-2157-40c6-80bb-9ab1a50c8ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备训练数据\n",
    "def dataset(train_words: list[list[str]], device=\"cpu\"):\n",
    "    all_tokens = []\n",
    "    all_tags = []\n",
    "    for sent in train_words:\n",
    "        tokens = []\n",
    "        tags = []\n",
    "        for word in sent:\n",
    "            tokens.extend(tokenizer.token2id[c] for c in word)\n",
    "            tags.extend(tokenizer.tag2id[tag] for tag in bieos_dataset[word])\n",
    "        assert len(tokens) == len(tags)\n",
    "        all_tokens.append(torch.tensor(tokens, device=device))\n",
    "        all_tags.append(torch.tensor(tags, device=device))\n",
    "    return TensorDataset(\n",
    "        pad_sequence(all_tokens, batch_first=True),\n",
    "        pad_sequence(all_tags, batch_first=True),\n",
    "    )\n",
    "\n",
    "\n",
    "# 加载数据集\n",
    "dataloader = DataLoader(dataset(train_words, DEVICE), batch_size=1024, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2323df9-daf0-4a5c-965e-aba1b3fdc52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备测试代码\n",
    "def evaluate_f1(pred_words, gold_words):\n",
    "    pred_counter = Counter()\n",
    "    gold_counter = Counter()\n",
    "    correct_counter = Counter()\n",
    "\n",
    "    for pred_sent, gold_sent in zip(pred_words, gold_words):\n",
    "        # 统计词频（以词为单位）\n",
    "        pred_counter.update(pred_sent)\n",
    "        gold_counter.update(gold_sent)\n",
    "        # 统计正确匹配的词\n",
    "        correct_words = set(pred_sent) & set(gold_sent)\n",
    "        correct_counter.update(correct_words)\n",
    "\n",
    "    precision = sum(correct_counter.values()) / sum(pred_counter.values())\n",
    "    recall = sum(correct_counter.values()) / sum(gold_counter.values())\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    return precision, recall, f1\n",
    "\n",
    "\n",
    "def evaluate_oov(pred_words, gold_words, train_words):\n",
    "    # 从训练集构建已知词表\n",
    "    known_words = set()\n",
    "    for sent in train_words:\n",
    "        known_words.update(sent)\n",
    "\n",
    "    oov_total = 0\n",
    "    oov_correct = 0\n",
    "\n",
    "    for gold_sent, pred_sent in zip(gold_words, pred_words):\n",
    "        for gold_word in gold_sent:\n",
    "            if gold_word not in known_words:  # 判断是否为OOV\n",
    "                oov_total += 1\n",
    "                if gold_word in pred_sent:  # 检查是否被正确切分\n",
    "                    oov_correct += 1\n",
    "\n",
    "    oov_recall = oov_correct / oov_total if oov_total > 0 else 0\n",
    "    return oov_recall\n",
    "\n",
    "\n",
    "testlines = [\"\".join(line) for line in gold_words]\n",
    "\n",
    "\n",
    "def evaluate(cuts):\n",
    "    pred_words = [cuts(line) for line in testlines]\n",
    "    precision, recall, f1 = evaluate_f1(pred_words, gold_words)\n",
    "    oov_recall = evaluate_oov(pred_words, gold_words, train_words)\n",
    "    print(f\"F1: {f1:.4f}, OOV Recall: {oov_recall:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1dc8a58a-51dc-4ce4-b7df-9c0e0d26e1f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\karis\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.425 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.5747, OOV Recall: 0.5861\n"
     ]
    }
   ],
   "source": [
    "# 使用 jieba 分词器测试评估函数\n",
    "\n",
    "import jieba_fast as jieba\n",
    "\n",
    "evaluate(lambda line: list(jieba.cut(line)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fde26af3-3122-45e3-ae5b-4b71c418c539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建分词模型\n",
    "class BiLSTM_CRF(torch.nn.Module):\n",
    "    def __init__(self, tokenizer: Tokenizer, embed_dim=64, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tag_size = len(tokenizer.id2tag)\n",
    "        self.embedding = torch.nn.Embedding(len(tokenizer.id2token), embed_dim)\n",
    "        self.lstm = torch.nn.LSTM(embed_dim, hidden_dim // 2, bidirectional=True, batch_first=True)\n",
    "        self.hidden2tag = torch.nn.Linear(hidden_dim, self.tag_size)\n",
    "        self.transitions = torch.nn.Parameter(torch.randn(self.tag_size, self.tag_size))\n",
    "        # self.transitions.data[self.tokenizer.tag2id[\"B\"]][self.tokenizer.tag2id[\"S\"]] = -100.0  # 禁止B→S转移\n",
    "        # self.transitions.data[self.tokenizer.tag2id[\"S\"]][self.tokenizer.tag2id[\"E\"]] = -100.0  # 禁止S→E转移\n",
    "        # self.transitions.data[self.tokenizer.tag2id[\"S\"]][self.tokenizer.tag2id[\"I\"]] = -100.0  # 禁止S→I转移\n",
    "        self.transitions.data[:, self.tokenizer.tag2id[\"<START>\"]] = -100.0\n",
    "        # self.transitions.data[self.tokenizer.tag2id[\"<START>\"]][self.tokenizer.tag2id[\"B\"]] = 100.0\n",
    "        # self.transitions.data[self.tokenizer.tag2id[\"<STOP>\"], :] = -1000.0\n",
    "        # self.transitions.data[self.tokenizer.tag2id[\"<STOP>\"]][self.tokenizer.tag2id[\"<STOP>\"]] = 100.0\n",
    "\n",
    "    def lstm_features(self, sentence: torch.Tensor) -> torch.Tensor:\n",
    "        emb = self.embedding(sentence)\n",
    "        lengths = (sentence != self.tokenizer.token2id[\"[PAD]\"]).sum(dim=-1).cpu()\n",
    "        packed = pack_padded_sequence(emb, lengths, enforce_sorted=False, batch_first=True)\n",
    "        lstm_out, _ = self.lstm(packed)\n",
    "        lstm_out, _ = pad_packed_sequence(lstm_out, batch_first=True)\n",
    "        lstm_feats = self.hidden2tag(lstm_out)\n",
    "        return lstm_feats\n",
    "\n",
    "    def viterbi_decode(self, feats: torch.Tensor):\n",
    "        backpointers = []\n",
    "        forward_var = torch.full((1, model.tag_size), -2000.0, device=DEVICE)\n",
    "        forward_var[0][model.tokenizer.tag2id[\"<START>\"]] = 2000.0\n",
    "        # 初始化向前传播变量，并让 (全部 B)初始 forward_var 的预测标签为 START\n",
    "        # forward_var 的形状应该是 [B, tag_size]，但此时的形状是 [1,tag_size]\n",
    "        # 在第一次与 feat 相加时会把第一维被广播成 B\n",
    "        # feats:[B, T, tag_size], feat是遍历时间步,形状是 [B, tag_size]\n",
    "        for feat in feats.unbind(dim=1):\n",
    "            # bptrs_t = []  # bptrs_t: 保存了当前时间步的标签\n",
    "            # viterbivars_t = []  # viterbivars_t: 保存当前时间步的 viterbi 变量\n",
    "            # for iter_tag in range(self.tag_size):\n",
    "            #     # iter_tag 是遍历的所有的标签\n",
    "            #     iter_tag_var = forward_var + self.transitions[iter_tag]\n",
    "            #     # iter_tag_var 是前一步的分数加上转移到 iter_tag 的分数 (广播 self.transitions[iter_tag])\n",
    "            #     current_tag = torch.argmax(iter_tag_var, dim=-1)\n",
    "            #     # current_tag 是如果前序标签是 iter_tag 时，此时最有可能的标签\n",
    "            #     # current_tag 的形状是 [B]\n",
    "            #     bptrs_t.append(current_tag)\n",
    "            #     viterbivars_t.append(torch.gather(iter_tag_var, 1, current_tag.unsqueeze(-1)))\n",
    "            #     # 保存 iter_tag 转移到 current_tag 的分数\n",
    "            # backpointers.append(torch.stack(bptrs_t, dim=-1))\n",
    "            # # viterbivars_t 是 list[Tensor[B]] (len(viterbivars_t) == tag_size)。需要转换成 [B, tag_size] 形状的 viterbi 变量\n",
    "            # forward_var = torch.cat(viterbivars_t, dim=-1) + feat\n",
    "            expanded_forward = forward_var.unsqueeze(1).expand(-1, self.tag_size, -1)  # [B, tag_size, tag_size]\n",
    "            transitions = self.transitions.unsqueeze(0)  # [1, tag_size, tag_size]\n",
    "            iter_tag_vars = expanded_forward + transitions  # [B, tag_size, tag_size]\n",
    "            bptrs_t = torch.argmax(iter_tag_vars, dim=-1)  # [B, tag_size]\n",
    "            viterbivars_t = torch.gather(iter_tag_vars, 2, bptrs_t.unsqueeze(-1))  # [B, tag_size]\n",
    "            backpointers.append(bptrs_t)\n",
    "            forward_var = viterbivars_t.squeeze(-1) + feat\n",
    "        # 转移到STOP标签\n",
    "        terminal_var = forward_var + self.transitions[self.tokenizer.tag2id[\"<STOP>\"]]\n",
    "        terminal_tag = torch.argmax(terminal_var, dim=-1)\n",
    "        score = torch.gather(terminal_var, 1, terminal_tag.unsqueeze(-1))\n",
    "        # 回溯得到最佳路径\n",
    "        path = [terminal_tag]\n",
    "        current_tag = terminal_tag.unsqueeze(-1)  # 这里的 current_tag 是批量的,需要给批次内的每个标签都回溯\n",
    "        # 弹出开始标签\n",
    "        backpointers.pop(0)\n",
    "        for bptrs_t in reversed(backpointers):\n",
    "            # bptrs_t 的形状为 [B, tag_size]\n",
    "            current_tag = torch.gather(bptrs_t, 1, current_tag)\n",
    "            path.append(current_tag.squeeze(-1))\n",
    "        path.reverse()\n",
    "        return path, score\n",
    "\n",
    "    def forward(self, sentence: torch.Tensor):\n",
    "        feats = self.lstm_features(sentence)\n",
    "        path, score = self.viterbi_decode(feats)\n",
    "        return path, score\n",
    "\n",
    "    def tag(self, line: str):\n",
    "        tensor = torch.tensor(self.tokenizer.encode(line), device=DEVICE)\n",
    "        path, score = self(tensor.unsqueeze(0))\n",
    "        return [self.tokenizer.id2tag[index.item()] for index in path]\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def cut(self, line: str) -> list[str]:\n",
    "        words = []\n",
    "        current_word = []\n",
    "        for i, tag in enumerate(self.tag(line)):\n",
    "            match tag:\n",
    "                case \"B\":\n",
    "                    if current_word:  # 处理前一个未完成的词\n",
    "                        words.append(\"\".join(current_word))\n",
    "                        current_word = []\n",
    "                    current_word.append(line[i])\n",
    "                case \"I\" | \"O\":\n",
    "                    current_word.append(line[i])\n",
    "                case \"E\":\n",
    "                    current_word.append(line[i])\n",
    "                    words.append(\"\".join(current_word))\n",
    "                    current_word = []\n",
    "                case \"S\" | \"<STOP>\":\n",
    "                    if current_word:\n",
    "                        words.append(\"\".join(current_word))\n",
    "                        current_word = []\n",
    "                    words.append(line[i])\n",
    "        if current_word:\n",
    "            words.append(\"\".join(current_word))\n",
    "        return words\n",
    "\n",
    "model = BiLSTM_CRF(tokenizer).to(DEVICE) if False else torch.load(\"BiLSTM_CRF.pth\", weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81020175-c254-49b4-b103-ac5226d1f72a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B', 'E', 'B', 'E', 'B', 'E', 'S', 'S', 'B', 'E', 'O', 'O', 'B', 'O', 'O', 'S', 'E', 'B', 'E', 'B', 'E']\n",
      "['共同', '创造', '美好', '的', '新', '世纪', '——', '二○○', '一', '年', '新年', '贺词']\n",
      "共同创造美好的新世纪——二○○一年新年贺词\n",
      "F1: 0.6029, OOV Recall: 0.4644\n"
     ]
    }
   ],
   "source": [
    "# 测试模型输入输出是否正常，并进行训练前评估\n",
    "print(model.tag(testlines[0]))\n",
    "print(model.cut(testlines[0]))\n",
    "print(testlines[0])\n",
    "\n",
    "evaluate(lambda line: model.cut(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92417878-a232-407b-8497-4221d57efc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练模型\n",
    "\n",
    "epochs = 0\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for texts, tags in dataloader:\n",
    "        forward_var = torch.full((1, model.tag_size), -2000.0, device=DEVICE)\n",
    "        forward_var[0][model.tokenizer.tag2id[\"<START>\"]] = 2000.0\n",
    "        feats = model.lstm_features(texts)\n",
    "        score = torch.zeros(texts.size(0), device=DEVICE)\n",
    "        tags = torch.nn.functional.pad(tags, (1, 0), \"constant\", model.tokenizer.tag2id[\"<START>\"])\n",
    "        # 迭代句子中的每个词\n",
    "        for i, feat in enumerate(feats.unbind(dim=1)):\n",
    "            current_tag = tags[:, i + 1]\n",
    "            forward_tag = tags[:, i]\n",
    "            score += model.transitions[forward_tag, current_tag] + torch.gather(feat, 1, current_tag.unsqueeze(-1)).squeeze(-1)\n",
    "            # alphas_t = []\n",
    "            # for iter_tag in range(model.tag_size):\n",
    "            #     emit_score = feat[:, iter_tag].unsqueeze(-1).expand(-1, model.tag_size)\n",
    "            #     transition_score = model.transitions[iter_tag].unsqueeze(0)\n",
    "            #     iter_tag_var = forward_var + transition_score + emit_score\n",
    "            #     alphas_t.append(torch.logsumexp(iter_tag_var, dim=-1))\n",
    "            alphas_t = feat.unsqueeze(-1).expand(-1, -1, model.tag_size) + forward_var.unsqueeze(1) + model.transitions.unsqueeze(0)\n",
    "            forward_var = torch.logsumexp(alphas_t, dim=-1)\n",
    "        # 转移到STOP标签\n",
    "        terminal_var = forward_var + model.transitions[model.tokenizer.tag2id[\"<STOP>\"]]\n",
    "        alpha = torch.logsumexp(terminal_var, dim=-1)\n",
    "        # 计算给定标签序列的分数\n",
    "        loss = (alpha - score).mean()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
    "    if epoch % 10 == 9:\n",
    "        evaluate(lambda line: model.cut(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1be9d30f-4d4e-46c6-9c11-51668159e1ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我相信，只要全世界人民以及所有关心人类前途和命运的政治家们共同努力，携手前进，我们居住的这个星球一定能够成为各国人民共享和平、共同发展和共同进步的美好世界！\n",
      "['S', 'B', 'E', 'O', 'B', 'E', 'S', 'B', 'E', 'B', 'E', 'B', 'E', 'B', 'E', 'B', 'E', 'B', 'E', 'B', 'E', 'S', 'B', 'E', 'S', 'B', 'I', 'B', 'S', 'B', 'E', 'B', 'E', 'O', 'B', 'E', 'B', 'E', 'O', 'B', 'E', 'B', 'E', 'S', 'B', 'E', 'B', 'E', 'B', 'E', 'B', 'E', 'B', 'E', 'B', 'E', 'B', 'E', 'B', 'E', 'B', 'E', 'O', 'B', 'E', 'B', 'E', 'S', 'B', 'E', 'B', 'E', 'S', 'B', 'E', 'B', 'E', 'O']\n",
      "['我', '相信', '，', '只要', '全', '世界', '人民', '以及', '所有', '关心', '人类', '前途', '和', '命运', '的', '政治', '家', '们', '共同', '努力', '，', '携手', '前进', '，', '我们', '居住', '的', '这个', '星球', '一定', '能够', '成为', '各国', '人民', '共享', '和平', '、', '共同', '发展', '和', '共同', '进步', '的', '美好', '世界', '！']\n",
      "['我', '相信', '，', '只要', '全世界', '人民', '以及', '所有', '关心', '人类', '前途', '和', '命运', '的', '政治家', '们', '共同努力', '，', '携手前进', '，', '我们', '居住', '的', '这个', '星球', '一定', '能够', '成为', '各国', '人民', '共享', '和平', '、', '共同', '发展', '和', '共同进步', '的', '美好世界', '！']\n"
     ]
    }
   ],
   "source": [
    "print(testlines[11])\n",
    "print(model.tag(testlines[11]))\n",
    "print(model.cut(testlines[11]))\n",
    "print(list(jieba.cut(testlines[11])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38231dee-9cd8-434a-98e2-67eac5aecbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"BiLSTM_CRF.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1b5fc4-478d-482a-99f3-4e0c32db6e24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
