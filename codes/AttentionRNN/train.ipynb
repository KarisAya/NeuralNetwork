{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "334e5221-4df0-47c5-a333-e26c1cedf8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "from datasets import load_dataset, concatenate_datasets, Dataset\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5acc5345-eef8-4680-aa3c-11c46d557360",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionEDM(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        en_tokenizer: Tokenizer,\n",
    "        zh_tokenizer: Tokenizer,\n",
    "        emb_size: int,\n",
    "        hidden_size: int,\n",
    "        num_layers: int,\n",
    "        dropout: float = 0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.en_tokenizer = en_tokenizer\n",
    "        self.zh_tokenizer = zh_tokenizer\n",
    "        input_vs = en_tokenizer.get_vocab_size()\n",
    "        output_vs = zh_tokenizer.get_vocab_size()\n",
    "        self.bos_ind = zh_tokenizer.token_to_id(\"[BOS]\")\n",
    "        self.pad_ind = zh_tokenizer.token_to_id(\"[PAD]\")\n",
    "        self.eos_ind = zh_tokenizer.token_to_id(\"[EOS]\")\n",
    "        # 模型主体\n",
    "        self.enc_embedding = torch.nn.Embedding(input_vs, emb_size)\n",
    "        self.enc_lstm = torch.nn.LSTM(emb_size, hidden_size, num_layers, dropout=dropout, batch_first=True)\n",
    "        self.dec_embedding = torch.nn.Embedding(output_vs, hidden_size)\n",
    "        self.dec_lstm = torch.nn.LSTM(hidden_size, hidden_size, num_layers, dropout=dropout, batch_first=True)\n",
    "        self.lm = torch.nn.Linear(hidden_size, output_vs)\n",
    "\n",
    "    def forward(self, source: torch.Tensor, target: torch.Tensor, teacher_forcing_ratio: float = 0.5):\n",
    "        embed_source: torch.Tensor = self.enc_embedding(source)\n",
    "        encoder_hiddens = []\n",
    "        state = None\n",
    "        for x in embed_source.unbind(1):\n",
    "            _, (hidden, cell) = self.enc_lstm(x.unsqueeze(1), state)\n",
    "            state = (hidden, cell)\n",
    "            encoder_hiddens.append(hidden)\n",
    "        encoded = torch.stack(encoder_hiddens, dim=-2)\n",
    "        encoded_trans = torch.transpose(encoded, -2, -1)\n",
    "        sqrtT: float = encoded_trans.size(-1) ** 0.5\n",
    "        outputs = []\n",
    "        x = target[:, 0].unsqueeze(-1)\n",
    "        for target_t in target.unbind(1):\n",
    "            embed = self.dec_embedding(x)\n",
    "            scores = embed.unsqueeze(0) @ encoded_trans\n",
    "            scores = torch.softmax(scores / sqrtT, dim=-1)\n",
    "            hidden = scores @ encoded\n",
    "            dec_output, (hidden, cell) = self.dec_lstm(embed, (hidden.squeeze(-2), cell))\n",
    "            output = self.lm(dec_output)\n",
    "            outputs.append(output)\n",
    "            if teacher_forcing_ratio > 0 and torch.rand(1).item() < teacher_forcing_ratio:\n",
    "                x = target_t.unsqueeze(1)\n",
    "            else:\n",
    "                x = output.argmax(dim=-1)\n",
    "        return torch.cat(outputs, 1)\n",
    "\n",
    "    def translate(self, inputs: str, device=DEVICE, max_len_ratio=2.0):\n",
    "        encoded_inputs = [self.bos_ind] + self.en_tokenizer.encode(inputs).ids + [self.eos_ind]\n",
    "        source = torch.tensor(encoded_inputs, dtype=torch.long, device=device).unsqueeze(0)\n",
    "        embed_source: torch.Tensor = self.enc_embedding(source)\n",
    "        encoder_hiddens = []\n",
    "        state = None\n",
    "        for x in embed_source.unbind(1):\n",
    "            _, (hidden, cell) = self.enc_lstm(x.unsqueeze(1), state)\n",
    "            state = (hidden, cell)\n",
    "            encoder_hiddens.append(hidden)\n",
    "        encoded = torch.stack(encoder_hiddens, dim=-2)\n",
    "        encoded_trans = torch.transpose(encoded, -2, -1)\n",
    "        sqrtT: float = encoded_trans.size(-1) ** 0.5\n",
    "        outs = []\n",
    "        x = torch.tensor([self.bos_ind], dtype=torch.long, device=device).unsqueeze(0)\n",
    "        for _ in range(int(len(inputs) * max_len_ratio)):\n",
    "            embed = self.dec_embedding(x)\n",
    "            scores = embed.unsqueeze(0) @ encoded_trans\n",
    "            scores = torch.softmax(scores / sqrtT, dim=-1)\n",
    "            hidden = scores @ encoded\n",
    "            dec_output, (hidden, cell) = self.dec_lstm(embed, (hidden.squeeze(-2), cell))\n",
    "            output = self.lm(dec_output)\n",
    "            x = output.argmax(dim=-1)\n",
    "            xi = x[0].item()\n",
    "            if xi == self.bos_ind:\n",
    "                continue\n",
    "            if xi == self.pad_ind:\n",
    "                continue\n",
    "            if xi == self.eos_ind:\n",
    "                break\n",
    "            outs.append(xi)\n",
    "        return self.zh_tokenizer.decode(outs, skip_special_tokens=False).replace(\" ##\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "cfe9394e-841b-499e-b5cb-5c36067e999f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型参数量  10153384\n"
     ]
    }
   ],
   "source": [
    "model = torch.load(\"translator.pth\", weights_only=False)\n",
    "assert isinstance(model, AttentionEDM)\n",
    "\n",
    "model.to(DEVICE)\n",
    "\n",
    "print(\"模型参数量 \", sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "85c4890d-49b4-4198-90f8-ac5b96151d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我爱你\n",
      "所有的道路都变成了罗马\n"
     ]
    }
   ],
   "source": [
    "print(model.translate(\"I love You\"))\n",
    "print(model.translate(\"All roads lead to Rome\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8195d70f-5d25-40ad-b3a4-e1b3195976be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_processing(dataset: Dataset):\n",
    "    def chinese_rule(code: int):\n",
    "        return code < 128 or 0x4E00 <= code <= 0x9FFF or 0x3000 <= code <= 0x303F or 0xFF01 <= code <= 0xFF5E\n",
    "\n",
    "    def english_rule(code: int):\n",
    "        return code < 128 or 0x3000 <= code <= 0x303F or 0xFF01 <= code <= 0xFF5E\n",
    "\n",
    "    en_list = []\n",
    "    zh_list = []\n",
    "    for data in tqdm(dataset, desc=\"pre-processing\"):\n",
    "        en = data[\"english\"]\n",
    "        zh = data[\"non_english\"]\n",
    "        if all([english_rule(ord(c)) for c in en]) and all([chinese_rule(ord(c)) for c in zh]):\n",
    "            en_list.append(en)\n",
    "            zh_list.append(zh)\n",
    "\n",
    "    print(f\"训练文本长度: {len(en_list)}\")\n",
    "\n",
    "    en_dataset = [[2] + model.en_tokenizer.encode(en).ids + [3] for en in en_list]\n",
    "    zh_dataset = [[2] + model.zh_tokenizer.encode(zh).ids + [3] for zh in zh_list]\n",
    "    return en_dataset, zh_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9d0881-f74c-4cf6-8b2b-415d16bad14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikimatrix = load_dataset(\"sentence-transformers/parallel-sentences-wikimatrix\", \"en-zh\")[\"train\"]\n",
    "opensubtitles = load_dataset(\"sentence-transformers/parallel-sentences-opensubtitles\", \"en-zh_cn\")[\"train\"]\n",
    "talks = load_dataset(\"sentence-transformers/parallel-sentences-talks\", \"en-zh-cn\")[\"train\"]\n",
    "tatoeba = load_dataset(\"sentence-transformers/parallel-sentences-tatoeba\", \"en-zh\")[\"train\"]\n",
    "# ccmatrix = load_dataset(\"sentence-transformers/parallel-sentences-ccmatrix\", \"en-zh\")[\"train\"]\n",
    "\n",
    "dataset = concatenate_datasets([wikimatrix, opensubtitles, talks, tatoeba])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0d2b4f-2c96-4802-84df-04b961b49be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_dataset, zh_dataset = dataset_processing(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4155100b-fbc2-41dd-b915-b46c5fd12135",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "def batch_processing(dataset: list[list[int]], batch_size: int, device=DEVICE):\n",
    "    return [\n",
    "        pad_sequence([torch.tensor(x, dtype=torch.long, device=device) for x in dataset[i : i + batch_size]], batch_first=True)\n",
    "        for i in range(0, len(dataset), batch_size)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18fb34a-5691-4175-b872-ebb9b8c37b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_dataset = batch_processing(en_dataset, 32)\n",
    "zh_dataset = batch_processing(zh_dataset, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9a5ba5-de2f-40f1-a8ce-ed99254bf58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = list(zip(en_dataset, zh_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826ce3ce-cb53-4fde-980d-9cdc8d87c561",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 训练循环\n",
    "train_dataset_length = len(train_dataset)\n",
    "\n",
    "start_rate = 1.2\n",
    "epoch_rate = 0.1\n",
    "step_rate = epoch_rate / train_dataset_length\n",
    "\n",
    "num_epochs = 20\n",
    "lossi = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    random.shuffle(train_dataset)\n",
    "    for step, (en_batch, zh_batch) in enumerate(tqdm(train_dataset, desc=f\"Epoch {epoch+1}/{num_epochs}\")):\n",
    "        tf_rate = start_rate - epoch * epoch_rate - step * step_rate\n",
    "        outputs = model.forward(en_batch, zh_batch, tf_rate)\n",
    "        loss = torch.nn.functional.cross_entropy(outputs.transpose(-2, -1), zh_batch)\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss = loss.item()\n",
    "        lossi.append(loss)\n",
    "        total_loss += loss\n",
    "    torch.save(model, f\"translator-epoch{epoch}.pth\")\n",
    "    print(f\"Average Loss {total_loss/train_dataset_length:.4f}\")\n",
    "\n",
    "print(model.translate(\"hello world\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20118997-427e-43d5-be20-7b6f0fbd465b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.translate(\"site packages\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedb8c30-d1d3-4f94-8cd5-2faa9627ad16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
